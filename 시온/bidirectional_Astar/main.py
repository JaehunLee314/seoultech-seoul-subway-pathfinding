import pandas as pd, glob, os, heapq, math
from collections import defaultdict, deque
import time
import numpy as np
import random

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0.  ì§€í•˜ì²  ì—­ ë¦¬ìŠ¤íŠ¸ (1~5í˜¸ì„ )
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
line1_stations = {
    "trunk": [
        "ì†Œìš”ì‚°","ë™ë‘ì²œ","ë³´ì‚°","ë™ë‘ì²œì¤‘ì•™","ì§€í–‰","ë•ì •","ë•ê³„","ì–‘ì£¼","ë…¹ì–‘","ê°€ëŠ¥","ì˜ì •ë¶€",
        "íšŒë£¡","ë§ì›”ì‚¬","ë„ë´‰ì‚°","ë„ë´‰","ë°©í•™","ì°½ë™","ë…¹ì²œ","ì›”ê³„","ì„±ë¶","ì„ê³„","ì‹ ì´ë¬¸",
        "ì™¸ëŒ€ì•","íšŒê¸°","ì²­ëŸ‰ë¦¬","ì œê¸°ë™","ì‹ ì„¤ë™","ë™ë¬˜ì•","ë™ëŒ€ë¬¸","ì¢…ë¡œ5ê°€","ì¢…ë¡œ3ê°€","ì¢…ê°",
        "ì‹œì²­","ì„œìš¸ì—­","ë‚¨ì˜","ìš©ì‚°","ë…¸ëŸ‰ì§„","ëŒ€ë°©","ì‹ ê¸¸","ì˜ë“±í¬","ì‹ ë„ë¦¼","êµ¬ë¡œ",
        "ê°€ì‚°ë””ì§€í„¸ë‹¨ì§€","ë…ì‚°","ê¸ˆì²œêµ¬ì²­","ì„ìˆ˜","ê´€ì•…","ì•ˆì–‘","ëª…í•™","ê¸ˆì •","êµ°í¬","ë‹¹ì •",
        "ì˜ì™•","ì„±ê· ê´€ëŒ€","í™”ì„œ","ìˆ˜ì›","ì„¸ë¥˜","ë³‘ì ","ì„¸ë§ˆ","ì˜¤ì‚°ëŒ€","ì˜¤ì‚°","ì§„ìœ„","ì†¡íƒ„",
        "ì„œì •ë¦¬","ì§€ì œ","í‰íƒ","ì„±í™˜","ì§ì‚°","ë‘ì •","ì²œì•ˆ","ë´‰ëª…","ìŒìš©","ì•„ì‚°","ë°°ë°©",
        "ì˜¨ì–‘ì˜¨ì²œ","ì‹ ì°½"
    ],
    "incheon_branch": [
        "êµ¬ë¡œ","êµ¬ì¼","ê°œë´‰","ì˜¤ë¥˜ë™","ì˜¨ìˆ˜","ì—­ê³¡","ì†Œì‚¬","ë¶€ì²œ","ì¤‘ë™","ì†¡ë‚´","ë¶€ê°œ",
        "ë¶€í‰","ë°±ìš´","ë™ì•”","ê°„ì„","ì£¼ì•ˆ","ë„í™”","ì œë¬¼í¬","ë„ì›","ë™ì¸ì²œ","ì¸ì²œ"
    ],
    "gwangmyeong_branch": ["ê¸ˆì²œêµ¬ì²­","ê´‘ëª…"],
    "seodongtan_branch": ["ë³‘ì ","ì„¸ë§ˆ","ì„œë™íƒ„"]
}
line2_stations = {
    "main_loop": [
        "ì‹œì²­","ì„ì§€ë¡œì…êµ¬","ì„ì§€ë¡œ3ê°€","ì„ì§€ë¡œ4ê°€","ë™ëŒ€ë¬¸ì—­ì‚¬ë¬¸í™”ê³µì›","ì‹ ë‹¹","ìƒì™•ì‹­ë¦¬",
        "ì™•ì‹­ë¦¬","í•œì–‘ëŒ€","ëšì„¬","ì„±ìˆ˜","ê±´ëŒ€ì…êµ¬","êµ¬ì˜","ê°•ë³€","ì ì‹¤ë‚˜ë£¨","ì ì‹¤","ì ì‹¤ìƒˆë‚´",
        "ì¢…í•©ìš´ë™ì¥","ì‚¼ì„±","ì„ ë¦‰","ì—­ì‚¼","ê°•ë‚¨","êµëŒ€","ì„œì´ˆ","ë°©ë°°","ì‚¬ë‹¹","ë‚™ì„±ëŒ€",
        "ì„œìš¸ëŒ€ì…êµ¬","ë´‰ì²œ","ì‹ ë¦¼","ì‹ ëŒ€ë°©","êµ¬ë¡œë””ì§€í„¸ë‹¨ì§€","ëŒ€ë¦¼","ì‹ ë„ë¦¼","ë¬¸ë˜",
        "ì˜ë“±í¬êµ¬ì²­","ë‹¹ì‚°","í•©ì •","í™ëŒ€ì…êµ¬","ì‹ ì´Œ","ì´ëŒ€","ì•„í˜„","ì¶©ì •ë¡œ"
    ],
    "seongsu_branch": ["ì„±ìˆ˜","ìš©ë‹µ","ì‹ ë‹µ","ìš©ë‘","ì‹ ì„¤ë™"],
    "sinjeong_branch": ["ì‹ ë„ë¦¼","ë„ë¦¼ì²œ","ì–‘ì²œêµ¬ì²­","ì‹ ì •ë„¤ê±°ë¦¬","ê¹Œì¹˜ì‚°"]
}
line3_stations = [
    "ëŒ€í™”","ì£¼ì—½","ì •ë°œì‚°","ë§ˆë‘","ë°±ì„","ëŒ€ê³¡","í™”ì •","ì›ë‹¹","ì›í¥","ì‚¼ì†¡","ì§€ì¶•","êµ¬íŒŒë°œ",
    "ì—°ì‹ ë‚´","ë¶ˆê´‘","ë…¹ë²ˆ","í™ì œ","ë¬´ì•…ì¬","ë…ë¦½ë¬¸","ê²½ë³µê¶","ì•ˆêµ­","ì¢…ë¡œ3ê°€","ì„ì§€ë¡œ3ê°€",
    "ì¶©ë¬´ë¡œ","ë™ëŒ€ì…êµ¬","ì•½ìˆ˜","ê¸ˆí˜¸","ì˜¥ìˆ˜","ì••êµ¬ì •","ì‹ ì‚¬","ì ì›","ê³ ì†í„°ë¯¸ë„","êµëŒ€",
    "ë‚¨ë¶€í„°ë¯¸í„°ë„","ì–‘ì¬","ë§¤ë´‰","ë„ê³¡","ëŒ€ì¹˜","í•™ì—¬ìš¸","ëŒ€ì²­","ì¼ì›","ìˆ˜ì„œ","ê°€ë½ì‹œì¥",
    "ê²½ì°°ë³‘ì›","ì˜¤ê¸ˆ"
]
line4_stations = [
    "ì§„ì ‘ê´‘ë¦‰ìˆ²", "ì˜¤ë‚¨ì—­", "í’ì–‘ì—­", "ë³„ë‚´ë³„ê°€ëŒì—­", "ë‹¹ê³ ê°œ","ìƒê³„","ë…¸ì›","ì°½ë™","ìŒë¬¸","ìˆ˜ìœ ","ë¯¸ì•„","ë¯¸ì•„ì‚¬ê±°ë¦¬","ê¸¸ìŒ","ì„±ì‹ ì—¬ëŒ€ì…êµ¬",
    "í•œì„±ëŒ€ì…êµ¬","í˜œí™”","ë™ëŒ€ë¬¸","ë™ëŒ€ë¬¸ì—­ì‚¬ë¬¸í™”ê³µì›","ì¶©ë¬´ë¡œ","ëª…ë™","íšŒí˜„","ì„œìš¸ì—­",
    "ìˆ™ëŒ€ì…êµ¬","ì‚¼ê°ì§€","ì‹ ìš©ì‚°","ì´ì´Œ","ë™ì‘","ì´ìˆ˜","ì‚¬ë‹¹","ë‚¨íƒœë ¹","ì„ ë°”ìœ„","ê²½ë§ˆê³µì›",
    "ëŒ€ê³µì›","ê³¼ì²œ","ì •ë¶€ê³¼ì²œì²­ì‚¬","ì¸ë•ì›","í‰ì´Œ","ë²”ê³„","ê¸ˆì •","ì‚°ë³¸","ìˆ˜ë¦¬ì‚°","ëŒ€ì•¼ë¯¸",
    "ë°˜ì›”","ìƒë¡ìˆ˜","í•œëŒ€ì•","ì¤‘ì•™","ê³ ì”","ì´ˆì§€","ì•ˆì‚°","ì‹ ê¸¸ì˜¨ì²œ","ì •ì™•","ì˜¤ì´ë„"
]
line5_stations = {
    "main_line": [
        "ë°©í™”","ê°œí™”ì‚°","ê¹€í¬ê³µí•­","ì†¡ì •","ë§ˆê³¡","ë°œì‚°","ìš°ì¥ì‚°","í™”ê³¡","ê¹Œì¹˜ì‚°","ì‹ ì •","ëª©ë™",
        "ì˜¤ëª©êµ","ì–‘í‰","ì˜ë“±í¬êµ¬ì²­","ì˜ë“±í¬ì‹œì¥","ì‹ ê¸¸","ì—¬ì˜ë„","ì—¬ì˜ë‚˜ë£¨","ë§ˆí¬","ê³µë•","ì• ì˜¤ê°œ",
        "ì¶©ì •ë¡œ","ì„œëŒ€ë¬¸","ê´‘í™”ë¬¸","ì¢…ë¡œ3ê°€","ì„ì§€ë¡œ4ê°€","ë™ëŒ€ë¬¸ì—­ì‚¬ë¬¸í™”ê³µì›","ì²­êµ¬","ì‹ ê¸ˆí˜¸",
        "í–‰ë‹¹","ì™•ì‹­ë¦¬","ë§ˆì¥","ë‹µì‹­ë¦¬","ì¥í•œí‰","êµ°ì","ì•„ì°¨ì‚°","ê´‘ë‚˜ë£¨","ì²œí˜¸","ê°•ë™",
        "ê¸¸ë™","êµ½ì€ë‹¤ë¦¬","ëª…ì¼","ê³ ë•","ìƒì¼ë™","ê°•ì¼","ë¯¸ì‚¬","í•˜ë‚¨í’ì‚°","í•˜ë‚¨ì‹œì²­","í•˜ë‚¨ê²€ë‹¨ì‚°"
    ],
    "macheon_branch": ["ê°•ë™","ë‘”ì´Œë™","ì˜¬ë¦¼í”½ê³µì›","ë°©ì´","ì˜¤ê¸ˆ","ê°œë¡±","ê±°ì—¬","ë§ˆì²œ"]
}
lines = {1: line1_stations, 2: line2_stations, 3: line3_stations,
         4: line4_stations, 5: line5_stations}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê³µí†µ ìƒìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_TRAVEL    = 2        # ì—­ê°„ ë°ì´í„° ì—†ì„ ë•Œ ê¸°ë³¸ ì£¼í–‰ ì‹œê°„ (ë¶„)
FALLBACK_TRANSFER = 4        # í™˜ìŠ¹ CSVì— ì—†ì„ ê²½ìš° ì‚¬ìš©í•  ê¸°ë³¸ í™˜ìŠ¹ ì‹œê°„ (ë¶„)
DWELL             = 0.5      # ê° ì •ê±°ì¥ ë„ì°© ì‹œ ì •ì°¨ ì‹œê°„: 0.5ë¶„ (=30ì´ˆ)
RUN_DIR           = "dataset/ì—­ê°„ì†Œìš”ì‹œê°„(ìˆ˜ì‘ì—…)"
TRANSFER_CSV_PATH = "dataset/ì„œìš¸êµí†µê³µì‚¬_í™˜ìŠ¹ì—­ê±°ë¦¬ ì†Œìš”ì‹œê°„ ì •ë³´_20250331.csv"
COORD_CSV_PATH    = "dataset/station_location"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í—¬í¼ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _parse_mmss(txt: str):
    try:
        m, s = map(int, txt.split(":"))
        return m + s/60
    except Exception:
        return None

def load_run_times(folder=RUN_DIR) -> dict[tuple[str, str], float]:
    run = {}
    for p in glob.glob(os.path.join(folder, "*.csv")):
        df = pd.read_csv(p, encoding="utf-8")
        prev = None
        for _, row in df.iterrows():
            cur = str(row["ì—­ëª…"]).strip()
            t = _parse_mmss(str(row["ì‹œê°„(ë¶„)"]).strip())
            if t is None:
                prev = cur
                continue
            if prev:
                run[(prev, cur)] = run[(cur, prev)] = t
            prev = cur
    return run

def load_transfer_times(csv_path=TRANSFER_CSV_PATH):
    tbl = {}
    if not os.path.exists(csv_path):
        return tbl
    df = pd.read_csv(csv_path, encoding="cp949")
    for _, row in df.iterrows():
        ln1 = int(row["í˜¸ì„ "])
        st  = str(row["í™˜ìŠ¹ì—­ëª…"]).strip()
        digits = "".join(filter(str.isdigit, str(row["í™˜ìŠ¹ë…¸ì„ "])))
        if not digits: continue
        ln2 = int(digits)
        t = _parse_mmss(str(row["í™˜ìŠ¹ì†Œìš”ì‹œê°„"]).strip())
        if t is None: continue
        tbl[((ln1,st),(ln2,st))] = tbl[((ln2,st),(ln1,st))] = t
    return tbl

def load_coords(folder_path=COORD_CSV_PATH):
    coords = {}
    for file in glob.glob(os.path.join(folder_path, "*.csv")):
        try:
            df = pd.read_csv(file, encoding="utf-8")
        except Exception as e:
            print(f"âš ï¸ CSV íŒŒì¼ {file} ì—´ê¸° ì‹¤íŒ¨: {e}")
            continue

        for _, row in df.iterrows():
            try:
                line_str = str(row["line"]).strip()
                if not line_str.endswith("í˜¸ì„ "):
                    continue
                ln = int(line_str.replace("í˜¸ì„ ", ""))
                station = str(row["station_name"]).strip()
                lat = float(row["latitude"])
                lon = float(row["longitude"])
                coords[(ln, station)] = (lat, lon)
            except Exception:
                continue  # í˜•ì‹ì´ ë§ì§€ ì•ŠëŠ” rowëŠ” ìŠ¤í‚µ
    return coords

def haversine(lat1, lon1, lat2, lon2):
    R = 6371.0
    dlat, dlon = math.radians(lat2-lat1), math.radians(lon2-lon1)
    a = math.sin(dlat/2)**2 + math.cos(math.radians(lat1))*math.cos(math.radians(lat2))*math.sin(dlon/2)**2
    return 2*R*math.asin(math.sqrt(a))  # km

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê·¸ë˜í”„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_graph(lines_dict, run_tbl, transfer_tbl):
    g = defaultdict(list)
    def add(u,v,w):
        g[u].append((w,v)); g[v].append((w,u))

    for ln,data in lines_dict.items():
        segments = data.values() if isinstance(data,dict) else [data]
        for key, seg in (data.items() if isinstance(data,dict) else [("linear",data)]):
            for a,b in zip(seg, seg[1:]):
                base = run_tbl.get((a,b), run_tbl.get((b,a), DEFAULT_TRAVEL))
                add((ln,a),(ln,b), base+DWELL)
            if key.endswith("_loop"):
                a,b=seg[-1],seg[0]
                base = run_tbl.get((a,b), run_tbl.get((b,a), DEFAULT_TRAVEL))
                add((ln,a),(ln,b), base+DWELL)

    st_to_lines=defaultdict(list)
    for ln,data in lines_dict.items():
        sts=(s for seg in data.values() for s in seg) if isinstance(data,dict) else data
        for st in sts:
            st_to_lines[st].append(ln)
    for st,lns in st_to_lines.items():
        for i in range(len(lns)):
            for j in range(i+1,len(lns)):
                u,v=(lns[i],st),(lns[j],st)
                w=transfer_tbl.get((u,v),FALLBACK_TRANSFER)
                add(u,v,w)
    return g

RUN_TIMES, TRANSFER_TIMES = load_run_times(), load_transfer_times()
COORDS = load_coords()
graph  = build_graph(lines, RUN_TIMES, TRANSFER_TIMES)

station_to_nodes=defaultdict(list)
for node in graph:
    station_to_nodes[node[1]].append(node)

MIN_EDGE = min(w for u in graph for w,_ in graph[u])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Bidirectional A* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def heuristic(u:tuple[int,str], v:tuple[int,str]):
    """ì§ì„ ê±°ë¦¬ ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹±(ë¶„). ì¢Œí‘œ ì—†ìœ¼ë©´ MIN_EDGE ì‚¬ìš©."""
    if u not in COORDS or v not in COORDS:
        return MIN_EDGE
    lat1,lon1 = COORDS[u]; lat2,lon2 = COORDS[v]
    return haversine(lat1,lon1,lat2,lon2)/30*60  # 30 km/h ê°€ì • â†’ ë¶„

def reconstruct(meet, parent_f, parent_b):
    # forward path
    path = []
    cur = meet
    while cur is not None:
        path.append(cur)
        cur = parent_f[cur]
    path = path[::-1]
    # backward path
    cur = parent_b[meet]
    while cur is not None:
        path.append(cur)
        cur = parent_b[cur]
    return path

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Bidirectional A* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def bidir_astar(start: str, goal: str):
    starts = station_to_nodes[start]
    goals  = station_to_nodes[goal]

    # forward = A*, backward = Dijkstra
    h_f = lambda u: min(heuristic(u, g) for g in goals)
    h_b = lambda u: 0

    # (f = g + h, g, node)
    pq_f = [(h_f(n), 0, n) for n in starts]
    pq_b = [(h_b(n), 0, n) for n in goals]
    heapq.heapify(pq_f)
    heapq.heapify(pq_b)

    g_f = {n: 0 for n in starts}
    g_b = {n: 0 for n in goals}
    parent_f = {n: None for n in starts}
    parent_b = {n: None for n in goals}

    best = float('inf')
    meet = None

    while pq_f and pq_b:
        # ë‹¤ìŒ í™•ì¥í•  ìª½ì„ fê°€ ì‘ì€ ìª½ìœ¼ë¡œ ì„ íƒ
        if pq_f[0][0] <= pq_b[0][0]:
            f_val, g_val, u = heapq.heappop(pq_f)
            if g_val > g_f[u]:
                continue
            # ì´ ì§€ì ì´ backwardì—ì„œ ì´ë¯¸ í™•ì¥ëœ ì ì´ ìˆê³ ,
            # g_f[u]+g_b[u] ê°€ í˜„ì¬ bestë³´ë‹¤ ì‘ë‹¤ë©´ meet í›„ë³´ë¡œ
            if u in g_b and (g_f[u] + g_b[u] < best):
                best, meet = g_f[u] + g_b[u], u
            # ê·¸ë¦¬ê³  ì•„ì§ bestë³´ë‹¤ ì‘ë‹¤ë©´ frontier í™•ì¥
            if g_val < best:
                for w, v in graph[u]:
                    ng = g_val + w
                    if ng < g_f.get(v, float('inf')):
                        g_f[v] = ng
                        parent_f[v] = u
                        heapq.heappush(pq_f, (ng + h_f(v), ng, v))
        else:
            f_val, g_val, u = heapq.heappop(pq_b)
            if g_val > g_b[u]:
                continue
            if u in g_f and (g_f[u] + g_b[u] < best):
                best, meet = g_f[u] + g_b[u], u
            if g_val < best:
                for w, v in graph[u]:
                    ng = g_val + w
                    if ng < g_b.get(v, float('inf')):
                        g_b[v] = ng
                        parent_b[v] = u
                        heapq.heappush(pq_b, (ng + h_b(v), ng, v))

        # ì¢…ë£Œ ì¡°ê±´: ì•ìœ¼ë¡œ ì–‘ìª½ì´ ë»—ì–´ë‚˜ê°ˆ ìµœì†Œ f í•©ì´ best ì´ìƒì´ë©´ ë” ì´ìƒ ê°œì„  ë¶ˆê°€ëŠ¥
        if best < float('inf') and pq_f and pq_b:
            if pq_f[0][0] + pq_b[0][0] >= best:
                break

    if meet is None:
        return None, float('inf')

    # ê²½ë¡œ ë³µì›
    path = []
    cur = meet
    while cur:
        path.append(cur)
        cur = parent_f[cur]
    path = path[::-1]
    cur = parent_b[meet]
    while cur:
        path.append(cur)
        cur = parent_b[cur]
    return path, best

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì¶œë ¥ ë³´ì¡° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5. ì¶œë ¥ ë³´ì¡° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def edge_time(u, v):
    for w, n in graph[u]:
        if n == v:
            return w
    return 0

def fmt_path(path):
    segs = []
    for i, (ln, st) in enumerate(path):
        if i == 0:
            segs.append(f"{ln}í˜¸ì„  {st}")
        else:
            prev = path[i - 1]
            segs.append(f" --{edge_time(prev, (ln, st)):.1f}ë¶„â†’ {ln}í˜¸ì„  {st}")
    return "".join(segs)

def generate_station_list():
    all_stations = set()

    def flatten_station_dict(d):
        for v in d.values():
            all_stations.update(v)

    flatten_station_dict(line1_stations)
    flatten_station_dict(line2_stations)
    all_stations.update(line3_stations)
    all_stations.update(line4_stations)
    flatten_station_dict(line5_stations)

    # âœ… í•­ìƒ ê°™ì€ ìˆœì„œë¡œ ì •ë ¬
    return sorted(list(all_stations))  # â† í•µì‹¬!

import time, random
import pandas as pd
import numpy as np

def analyse_1000_random_astar(all_stations, sample_count=100, repeats=30,
                              detail_csv="astar_runs.csv",
                              summary_csv="astar_summary.csv",
                              global_csv="global_summary.csv"):
    random.seed(42)
    # â¶ ìƒ˜í”Œ ìŒ ìƒì„±
    pairs = []
    seen = set()
    while len(pairs) < sample_count:
        s, g = random.sample(all_stations, 2)
        if (s, g) not in seen:
            pairs.append((s, g))
            seen.add((s, g))

    detail_records = []
    summary_records = []

    # â· ê° ìƒ˜í”Œë³„ ë°˜ë³µ ì‹¤í–‰ ë° summary ì¶œë ¥
    for i, (s, g) in enumerate(pairs, 1):
        times = []
        success = 0

        for r in range(1, repeats + 1):
            try:
                t0 = time.perf_counter()
                path, total = bidir_astar(s, g)
                t1 = time.perf_counter()
                if path:
                    elapsed = t1 - t0
                    times.append(elapsed)
                    success += 1
                    fmt = fmt_path(path)
                    detail_records.append({
                        "start": s, "goal": g, "run": r,
                        "elapsed_sec": round(elapsed, 6),
                        "total_min": round(total, 1),
                        "path": fmt
                    })
            except Exception:
                pass

        mean_sec  = float(np.mean(times)) if times else 0.0
        std_sec   = float(np.std(times))  if times else 0.0
        best_sec  = float(min(times))     if times else 0.0
        worst_sec = float(max(times))     if times else 0.0

        print(f"--- Sample {i}/{sample_count}: {s} â†’ {g} ìš”ì•½ ---")
        print(f"  ì‹œë„ íšŸìˆ˜  : {repeats}")
        print(f"  ì„±ê³µ íšŸìˆ˜  : {success}")
        print(f"  í‰ê·  ì‹œê°„  : {mean_sec:.6f}ì´ˆ")
        print(f"  í‘œì¤€í¸ì°¨   : {std_sec:.6f}ì´ˆ")
        print(f"  ìµœë‹¨ ì‹œê°„  : {best_sec:.6f}ì´ˆ")
        print(f"  ìµœì¥ ì‹œê°„  : {worst_sec:.6f}ì´ˆ\n")

        summary_records.append({
            "start": s,
            "goal": g,
            "runs_attempted": repeats,
            "runs_successful": success,
            "mean_sec": mean_sec,
            "std_sec": std_sec,
            "best_sec": best_sec,
            "worst_sec": worst_sec
        })

    # â¸ CSV ì €ì¥
    pd.DataFrame(detail_records).to_csv(detail_csv, index=False, encoding="utf-8-sig")
    pd.DataFrame(summary_records).to_csv(summary_csv, index=False, encoding="utf-8-sig")

    # â¹ ê¸€ë¡œë²Œ ìš”ì•½ ê³„ì‚°
    all_times = [rec["elapsed_sec"] for rec in detail_records]
    global_summary = {
        "total_runs":             len(all_times),
        "total_successful_runs":  sum(1 for rec in detail_records),
        "total_time_sec":         float(sum(all_times)),
        "mean_time_sec":          float(np.mean(all_times)) if all_times else 0.0,
        "std_time_sec":           float(np.std(all_times))  if all_times else 0.0,
        "best_time_sec":          float(min(all_times))     if all_times else 0.0,
        "worst_time_sec":         float(max(all_times))     if all_times else 0.0
    }

    # âº ê¸€ë¡œë²Œ ìš”ì•½ ì¶œë ¥
    print("=== ì „ì²´(Global) ìš”ì•½ ===")
    print(f"  ì‹œë„ ì´íšŸìˆ˜      : {global_summary['total_runs']}")
    print(f"  ì„±ê³µ ì´íšŸìˆ˜      : {global_summary['total_successful_runs']}")
    print(f"  ì „ì²´ ì†Œìš” ì‹œê°„  : {global_summary['total_time_sec']:.6f}ì´ˆ")
    print(f"  ì „ì²´ í‰ê·  ì‹œê°„  : {global_summary['mean_time_sec']:.6f}ì´ˆ")
    print(f"  ì „ì²´ í‘œì¤€í¸ì°¨    : {global_summary['std_time_sec']:.6f}ì´ˆ")
    print(f"  ì „ì²´ ìµœë‹¨ ì‹œê°„  : {global_summary['best_time_sec']:.6f}ì´ˆ")
    print(f"  ì „ì²´ ìµœì¥ ì‹œê°„  : {global_summary['worst_time_sec']:.6f}ì´ˆ\n")

    # â» ê¸€ë¡œë²Œ ìš”ì•½ CSV ì €ì¥
    pd.DataFrame([global_summary]).to_csv(global_csv, index=False, encoding="utf-8-sig")

    return summary_records, global_summary



if __name__ == "__main__":
    random.seed(42)

    all_stations = generate_station_list()

    print("ğŸš€ 1000ê°œ ëœë¤ ì¶œë°œ-ë„ì°© ìŒì— ëŒ€í•´ A* íƒìƒ‰ ì‹œì‘")
    # perâ€sample summary ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ìŠµë‹ˆë‹¤.
    summary_records = analyse_1000_random_astar(all_stations, sample_count=100)